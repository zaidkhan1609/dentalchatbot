# -*- coding: utf-8 -*-
"""Dental_Chatbot__Pipeline_working_mistral1 (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ctrQ1rrXPBmGftDtFuK2_ZvAt70SY57Y

# ðŸ¦· Fine-Tuned Dental Chatbot with CRAG and Streamlit UI

Welcome to the notebook for building a dental health chatbot pipeline using fine-tuned DistilGPT2 and CRAG.
This notebook includes:
- Fine-tuning a DistilGPT2 model on Q&A and multi-turn data.
- Building a FAISS-based CRAG system with fallback to DuckDuckGo.
- A Streamlit UI setup for interaction.
"""

!pip install -q sentence-transformers faiss-cpu transformers accelerate bitsandbytes duckduckgo_search datasets streamlit

!pip install -U sentence-transformers scikit-learn

hf_token = "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"

"""## Step 1: Load and Prepare Q&A and Multi-turn Dialogue Data"""

from huggingface_hub import login

login(token="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")

!pip uninstall -y bitsandbytes
!pip install bitsandbytes  # or the latest version from PyPI

import requests
import json

def load_json(url):
    r = requests.get(url)
    r.raise_for_status()
    return r.json()

def load_jsonl(url):
    r = requests.get(url)
    r.raise_for_status()
    return [json.loads(line) for line in r.text.splitlines() if line]

from sentence_transformers import SentenceTransformer
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
import joblib
import json

# âœ… Step 1: Load your merged training data
with open("merged_symptom_training_data.json", "r") as f:
    data = json.load(f)

texts = [item["text"] for item in data]
labels = [item["label"] for item in data]

# âœ… Step 2: Encode using Sentence-BERT
model_name = "all-MiniLM-L6-v2"
sbert = SentenceTransformer(model_name)
X = sbert.encode(texts, convert_to_tensor=False)

# âœ… Step 3: Train/test split
X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2, stratify=labels, random_state=42)

# âœ… Step 4: Train classifier
clf = LogisticRegression(max_iter=1000)
clf.fit(X_train, y_train)

# âœ… Step 5: Evaluate
y_pred = clf.predict(X_test)
print(classification_report(y_test, y_pred))

# âœ… Step 6: Save model and encoder
joblib.dump(clf, "symptom_classifier.pkl")
sbert.save("symptom_encoder")

print("âœ… Classifier and encoder saved.")

import joblib
from sentence_transformers import SentenceTransformer
import numpy as np

# âœ… Load saved model and encoder
clf = joblib.load("symptom_classifier.pkl")
sbert = SentenceTransformer("symptom_encoder")

# âœ… Classifier function
def classify_symptom(text):
    embedding = sbert.encode([text])
    prediction = clf.predict(embedding)[0]
    probas = clf.predict_proba(embedding)[0]
    confidence = np.max(probas)
    return prediction, float(confidence)
label, score = classify_symptom("My face is swollen and it hurts when I chew.")
print(f"Predicted symptom: {label} (confidence: {score:.2f})")

import json
import requests
from datasets import Dataset

# --- Helper functions to fetch data from GitHub ---
def load_json(url):
    r = requests.get(url)
    r.raise_for_status()
    return r.json()

def load_jsonl(url):
    r = requests.get(url)
    r.raise_for_status()
    return [json.loads(line) for line in r.text.splitlines() if line]

# --- URLs to your JSON files hosted on GitHub ---
base_url = "https://raw.githubusercontent.com/zaidkhan1609/Datasets/main/chatbot/"

single_turn_urls = {
    "dental_qa_pairs.json": base_url + "dental_qa_pairs.json",
    "dental_health_qna.json": base_url + "dental_health_qna.json",
    "brushing_and_flossing_qa.json": base_url + "brushing_and_flossing_qa.json",
    "reddit_qa.jsonl": base_url + "reddit_qa.jsonl"
}

multi_turn_url = base_url + "multi_turn_dialogues.json"

# --- Load single-turn QA data ---
qa_pairs = []
for fname, url in single_turn_urls.items():
    if fname.endswith(".jsonl"):
        qa_pairs.extend(load_jsonl(url))
    else:
        qa_pairs.extend(load_json(url))

# --- Load multi-turn dialogue data ---
multi_turn_data = load_json(multi_turn_url)

# --- Format into conversation examples ---
train_texts = []

# Single-turn: "User: ...\nBot: ..."
for item in qa_pairs:
    question = item['question'].strip()
    answer = item['answer'].strip()
    train_texts.append(f"User: {question}\nBot: {answer}")

# Multi-turn: "User: ...\nBot: ..."
for dialogue in multi_turn_data:
    convo_str = ""
    for turn in dialogue["turns"]:
        role = turn["role"].capitalize()
        text = turn["text"].strip()
        convo_str += f"{role}: {text}\n"
    train_texts.append(convo_str.strip())

print(f"Loaded {len(train_texts)} conversation examples for training.")

# --- Create Hugging Face dataset ---
dataset = Dataset.from_list([{"text": t} for t in train_texts])
dataset = dataset.train_test_split(test_size=0.1, seed=42)
print(f"Training samples: {len(dataset['train'])}, Validation samples: {len(dataset['test'])}")

import torch
torch.cuda.empty_cache()

# Upgrade PEFT to get BitFit support
!pip install -U peft

pip install -U bitsandbytes

# âœ… Install required libraries
!pip install -q transformers datasets peft accelerate requests

# âœ… Imports
import os, json, requests, torch
from datasets import Dataset
from transformers import (
    AutoModelForCausalLM, AutoTokenizer,
    TrainingArguments, Trainer, DataCollatorForLanguageModeling,
    BitsAndBytesConfig
)
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training

# âœ… Memory optimization
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
torch.cuda.empty_cache()

# âœ… Helper functions to load GitHub-hosted files
def load_json(url):
    r = requests.get(url)
    r.raise_for_status()
    return r.json()

def load_jsonl(url):
    r = requests.get(url)
    r.raise_for_status()
    return [json.loads(line) for line in r.text.splitlines() if line]

# âœ… GitHub base path
base_url = "https://raw.githubusercontent.com/zaidkhan1609/Datasets/main/chatbot/"

# âœ… Load QA data from GitHub
qa_pairs = []
qa_pairs += load_json(base_url + "dental_qa_pairs.json")
qa_pairs += load_json(base_url + "dental_health_qna.json")
qa_pairs += load_json(base_url + "brushing_and_flossing_qa.json")
qa_pairs += load_jsonl(base_url + "reddit_qa.jsonl")

# âœ… Load multi-turn dialogue from GitHub
multi_turn_data = load_json(base_url + "multi_turn_dialogues.json")

# âœ… Create formatted training texts
train_texts = [f"User: {item['question'].strip()}\nBot: {item['answer'].strip()}" for item in qa_pairs]
for dialogue in multi_turn_data:
    convo = "\n".join(f"{turn['role'].capitalize()}: {turn['text'].strip()}" for turn in dialogue["turns"])
    train_texts.append(convo)

# âœ… Tokenizer and model setup
model_id = "mistralai/Mistral-7B-Instruct-v0.2"
tokenizer = AutoTokenizer.from_pretrained(model_id)
tokenizer.pad_token = tokenizer.eos_token

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    llm_int8_enable_fp32_cpu_offload=True
)

model = AutoModelForCausalLM.from_pretrained(
    model_id,
    device_map="auto",
    quantization_config=bnb_config,
    torch_dtype=torch.float16
)
model = prepare_model_for_kbit_training(model)

# âœ… Apply LoRA config
lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.1,
    bias="none",
    task_type="CAUSAL_LM"
)
model = get_peft_model(model, lora_config)

# âœ… Tokenization
raw_dataset = Dataset.from_list([{"text": t} for t in train_texts])
dataset = raw_dataset.train_test_split(test_size=0.1, seed=42)

def tokenize(batch):
    return tokenizer(batch["text"], truncation=True, padding="max_length", max_length=512)

tokenized_dataset = dataset.map(tokenize, batched=True, remove_columns=["text"])
data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

# âœ… Training setup
training_args = TrainingArguments(
    output_dir="./phi2_lora_dental",
    num_train_epochs=3,
    per_device_train_batch_size=1,
    gradient_accumulation_steps=4,
    eval_strategy="epoch",
    save_strategy="epoch",
    learning_rate=2e-4,
    logging_steps=50,
    fp16=True,
    save_total_limit=2,
    report_to="none"
)

# âœ… Train
trainer = Trainer(
    model=model,
    tokenizer=tokenizer,
    args=training_args,
    data_collator=data_collator,
    train_dataset=tokenized_dataset["train"],
    eval_dataset=tokenized_dataset["test"]
)

trainer.train()

# âœ… Save fine-tuned model
model.save_pretrained("./phi2_lora_dental")
tokenizer.save_pretrained("./phi2_lora_dental")

print("âœ… LoRA fine-tuning complete! Model saved to ./phi2_lora_dental")



"""## Step 2: Fine-Tune DistilGPT2"""

import os
import json
import zipfile
import requests
from io import BytesIO
from sentence_transformers import SentenceTransformer

# âœ… GitHub base URL
base_url = "https://raw.githubusercontent.com/zaidkhan1609/Datasets/main/chatbot/"

# âœ… List of knowledge JSON files (hosted on GitHub)
knowledge_files = [
    "Tooth%20decay.json",
    "bleeding_gums.json",
    "knocked_out_tooth.json",
    "loose_tooth.json",
    "sensitive_teeth.json",
    "swelling_jaw.json",
    "tooth_pain.json",
    "vertopal.com_New%20Microsoft%20Word%20Document.json"
]

# âœ… Chunking function
def chunk_text(text, chunk_size=256, overlap=64):
    words = text.split()
    chunks = []
    for i in range(0, len(words), chunk_size - overlap):
        chunk = " ".join(words[i:i + chunk_size])
        if chunk:
            chunks.append(chunk)
    return chunks

# âœ… Helper to load and parse GitHub-hosted JSON
def extract_text_from_json_url(file_url):
    r = requests.get(file_url)
    r.raise_for_status()
    data = r.json()
    content_blocks = data[1] if isinstance(data, list) and len(data) > 1 else data
    texts = []

    for block in content_blocks:
        if not isinstance(block, dict) or "t" not in block:
            continue
        t = block["t"]
        if t in ("Para", "Plain", "Header"):
            inline_elems = block["c"] if t != "Header" else block["c"][2]
            paragraph_text = ""
            for inline in inline_elems:
                if inline["t"] == "Str":
                    paragraph_text += inline["c"]
                elif inline["t"] in ("Space", "SoftBreak"):
                    paragraph_text += " "
                elif inline["t"] == "LineBreak":
                    paragraph_text += "\n"
                elif inline["t"] == "Code":
                    paragraph_text += inline["c"][1]
            paragraph_text = paragraph_text.strip()
            if paragraph_text:
                texts.append(paragraph_text)
    return texts

# âœ… Load and extract all GitHub knowledge JSONs with chunking
knowledge_texts = []
for fname in knowledge_files:
    full_url = base_url + fname
    for text in extract_text_from_json_url(full_url):
        knowledge_texts.extend(chunk_text(text))  # âœ… CHUNKING applied here

# âœ… Also extract structured symptom tree info from ZIP
symptom_zip_url = base_url + "symptom_trees.zip"
r = requests.get(symptom_zip_url)
r.raise_for_status()

with zipfile.ZipFile(BytesIO(r.content)) as z:
    for filename in z.namelist():
        if filename.endswith(".json"):
            data = json.loads(z.read(filename).decode("utf-8"))
            symptom = data.get("symptom", "").capitalize()
            flow = data.get("flow", [])
            if not flow:
                continue
            advice_text = f"For {symptom}: "
            if len(flow) == 1:
                step = flow[0]
                if not all(k in step for k in ("question", "yes", "no")):
                    continue
                q = step["question"].rstrip("?")
                yes_outcome = step["yes"]
                no_outcome = step["no"]
                condition = q[0].lower() + q[1:]
                advice_text += f"If {condition}, then {yes_outcome}. Otherwise, {no_outcome}."
            else:
                for i, step in enumerate(flow):
                    if not all(k in step for k in ("question", "yes", "no")):
                        continue  # skip malformed step
                    q = step["question"].rstrip("?")
                    yes_outcome = step["yes"]
                    no_outcome = step["no"]
                    condition = q[0].lower() + q[1:]
                    if i < len(flow) - 1:
                        advice_text += f"If {condition}, then {yes_outcome}. Otherwise, "
                    else:
                        advice_text += f"If {condition}, then {yes_outcome}. Otherwise, {no_outcome}."
            knowledge_texts.append(advice_text)

# âœ… Preview and count
print(f"âœ… Total knowledge entries: {len(knowledge_texts)}")
print("ðŸ” Sample entry:\n", knowledge_texts[0][:300], "...")

import torch

device = "cuda" if torch.cuda.is_available() else "cpu"

from sentence_transformers.util import cos_sim
from duckduckgo_search import DDGS

def multiquery_variants(query):
    return [
        f"What is {query}?",
        f"Explain: {query}",
        f"{query} - symptoms and treatment",
        f"{query} dental advice",
        f"Causes and remedies for {query}"
    ]

def duckduckgo_fallback(query):
    with DDGS() as ddgs:
        results = [r["body"] for r in ddgs.text(query, max_results=3)]
    return "\\n".join(results)
# âœ… Load embedding model
from sentence_transformers import SentenceTransformer

embedding_model = SentenceTransformer("BAAI/bge-base-en-v1.5", device=device)

# âœ… Precompute knowledge base embeddings
kb_embeddings = embedding_model.encode(knowledge_texts, convert_to_tensor=True)
# Save knowledge base texts and embeddings for Streamlit app
import numpy as np

with open("knowledge_texts.json", "w") as f:
    json.dump(knowledge_texts, f, indent=2)

np.save("kb_embeddings.npy", kb_embeddings.cpu().numpy())

import torch
from torch.nn.functional import normalize  # âœ… NEW import

device = "cuda" if torch.cuda.is_available() else "cpu"

def get_crag_response(user_query, threshold=0.65):
    # MultiQuery expansion
    queries = multiquery_variants(user_query)

    # Encode queries and move to device
    query_embeddings = embedding_model.encode(queries, convert_to_tensor=True).to(device)

    # Normalize and move KB embeddings to same device
    kb_tensor = torch.tensor(kb_embeddings).to(device)
    kb_tensor = normalize(kb_tensor, dim=1)  # âœ… normalize all vectors (rows)

    best_score, best_text = float('-inf'), ""

    for qe in query_embeddings:
        qe = normalize(qe.unsqueeze(0), dim=1)  # âœ… normalize query vector
        scores = torch.matmul(qe, kb_tensor.T).squeeze(0)  # âœ… dot product similarity
        top_score, top_idx = torch.max(scores, dim=0)

        if top_score > best_score:
            best_score = top_score
            best_text = knowledge_texts[top_idx]

    # Fallback if similarity is too low
    if best_score < threshold:
        context = duckduckgo_fallback(user_query)
    else:
        context = best_text

    # Format prompt for chat model
    prompt = f"<s>[INST] Context: {context}\n\nUser: {user_query} [/INST]"
    input_ids = tokenizer(prompt, return_tensors="pt").input_ids.to(device)

    output = model.generate(input_ids, max_new_tokens=512)
    decoded = tokenizer.decode(output[0], skip_special_tokens=True)

    return decoded

print(get_crag_response("What are the signs of gum disease?"))

import streamlit as st
from io import BytesIO

# âœ… Load SDCEP triage decision trees from GitHub (symptom_trees.zip)
symptom_zip_url = base_url + "symptom_trees.zip"
r = requests.get(symptom_zip_url)
r.raise_for_status()
symptom_trees = {}
with zipfile.ZipFile(BytesIO(r.content)) as z:
    for filename in z.namelist():
        if filename.endswith(".json"):
            data = json.loads(z.read(filename).decode("utf-8"))
            symptom_name = data.get("symptom", "")
            flow = data.get("flow", [])
            if flow:
                # Store the flow list, using lowercase key for consistent lookup
                symptom_trees[symptom_name.lower()] = flow

# âœ… Function to traverse a symptom decision tree interactively
def traverse_sdcep_tree(symptom=None, user_input=None):
    """
    Traverse the SDCEP symptom tree for the given symptom. If symptom is provided (starting a new triage),
    it returns the first question for that symptom. If user_input is provided (continuing an ongoing triage),
    it processes the answer and returns the next question or final advice.
    """
    # Starting a new triage
    if symptom is not None:
        symptom_key = symptom.lower()
        if symptom_key not in symptom_trees:
            # No matching symptom tree found
            return get_crag_response(user_input or "")  # Fallback to normal response
        # Initialize session state for the new triage
        st.session_state["in_triage"] = True
        st.session_state["current_tree"] = symptom_key
        st.session_state["triage_index"] = 0
        st.session_state["triage_path"] = []  # to record Q&A path
        # Get the first question from the symptom's flow
        first_question = symptom_trees[symptom_key][0]["question"]
        st.session_state["last_question"] = first_question
        return first_question  # Ask the first question to the user

    # Continuing an existing triage (user_input is the answer to the last question)
    if not st.session_state.get("in_triage") or st.session_state.get("current_tree") is None:
        # If we're not actually in a triage flow, just fall back to CRAG
        return get_crag_response(user_input or "")

    # Retrieve current state
    symptom_key = st.session_state["current_tree"]
    flow = symptom_trees[symptom_key]
    idx = st.session_state.get("triage_index", 0)
    if idx >= len(flow):
        # Index out of range (should not normally happen)
        st.session_state["in_triage"] = False
        return get_crag_response(user_input or "")

    # Process the user's answer (expected yes/no)
    answer = (user_input or "").strip().lower()
    # Normalize common affirmative/negative responses
    yes_answers = {"yes", "y", "yeah", "yep", "yea", "sure"}
    no_answers  = {"no", "n", "nope", "nah", "negative"}
    if answer in yes_answers or answer.startswith("y"):
        user_answer = "yes"
    elif answer in no_answers or answer.startswith("n"):
        user_answer = "no"
    else:
        # If answer is not recognized as yes/no, prompt again
        return "Please answer 'yes' or 'no'."

    # Record this Q&A in the path
    question_asked = st.session_state.get("last_question", flow[idx]["question"])
    st.session_state["triage_path"].append({"question": question_asked, "answer": user_answer})

    # Determine next step based on answer
    current_node = flow[idx]
    next_step = current_node.get(user_answer)  # either the text of next question, or final advice tag
    if next_step is None:
        # No next step found (shouldn't happen if data is well-formed)
        st.session_state["in_triage"] = False
        return "Sorry, I couldn't determine the next step."

    # Check if next_step is a directive to jump to another pathway
    if isinstance(next_step, str) and next_step.lower().startswith("go to"):
        # Example: "Go to Swelling pathway" -> extract "swelling"
        target = next_step.split("Go to", 1)[1].strip().lower().replace(" pathway", "")
        # If the target pathway exists, switch to that tree
        if target in symptom_trees:
            st.session_state["current_tree"] = target
            st.session_state["triage_index"] = 0
            # Ask the first question of the new pathway
            first_question = symptom_trees[target][0]["question"]
            st.session_state["last_question"] = first_question
            return first_question
        else:
            # Target pathway not found; end triage
            st.session_state["in_triage"] = False
            return f"Unable to continue to the {target.capitalize()} pathway."

    # Check if next_step is a final outcome (triage decision) or another question
    # We consider it a final outcome if it contains "Care:" or is exactly "Urgent Care", "Emergency Care", etc.
    outcome_text = next_step.strip()
    is_final = False
    if outcome_text.lower().startswith(("emergency care", "urgent care", "self care", "non-urgent care")) or outcome_text.lower() == "urgent care" or outcome_text.lower() == "urgent care: dental":
        is_final = True
    if outcome_text.lower() == "end":
        is_final = True
        # Define a user-friendly message for "End"
        outcome_text = "No urgent issues identified. No immediate care needed."

    if is_final:
        # End of triage reached â€“ provide final advice
        st.session_state["in_triage"] = False  # reset triage state
        st.session_state["current_tree"] = None
        st.session_state["triage_index"] = None
        # Append the final outcome to the path (as the outcome of the last question)
        st.session_state["triage_path"].append({"question": "Triage Result", "answer": outcome_text})
        return outcome_text  # This is the final advice/triage result

    # If not final and not a pathway jump, treat next_step as the text of the next question in the current flow
    # Find the index of the question matching next_step in the flow list
    next_index = None
    next_q_lower = outcome_text.lower().rstrip("?")
    for i, step in enumerate(flow):
        # Match question text (case-insensitive, ignoring trailing '?')
        if step["question"].lower().rstrip("?") == next_q_lower:
            next_index = i
            break
    if next_index is None:
        # If we couldn't find the next question (data inconsistency), end triage
        st.session_state["in_triage"] = False
        return outcome_text  # Return what we have, which might be a message

    # Move to the next question
    st.session_state["triage_index"] = next_index
    next_question = flow[next_index]["question"]
    st.session_state["last_question"] = next_question
    # Return the next question to ask the user
    return next_question

# âœ… Routing function: decide between triage and general chat response
def get_chat_response(user_text):
    """Route the user query to either the triage engine or the general chat (CRAG) response."""
    # If a triage Q&A session is already in progress, continue it
    if st.session_state.get("in_triage"):
        # In the middle of a triage, the user_text is an answer to the last triage question
        return traverse_sdcep_tree(user_input=user_text)
    # Otherwise, determine if we should start a new triage or do a normal chat
    label, confidence = classify_symptom(user_text)
    label_key = label.lower()
    if confidence > 0.75 and label_key in symptom_trees:
        # High confidence that the query matches a symptom category â€“ start triage
        return traverse_sdcep_tree(symptom=label_key)
    else:
        # Low confidence or no matching symptom â€“ use the conversational QA response
        return get_crag_response(user_text)

# âœ… Streamlit App Interface
st.title("Dental Symptom Chatbot")

# Mode selection toggle
mode = st.radio("Choose Mode:", ("Chat", "Triage"), index=0)

# Initialize session state for triage (if not already set)
if "in_triage" not in st.session_state:
    st.session_state["in_triage"] = False
    st.session_state["current_tree"] = None
    st.session_state["triage_index"] = None
    st.session_state["triage_path"] = []

# User input field
user_query = st.text_input("Enter your question or describe your symptoms:")

if st.button("Send") and user_query:
    # Determine response based on mode and state
    if st.session_state["in_triage"]:
        # If already in a triage session, the user_query is an answer to the last question
        bot_reply = traverse_sdcep_tree(user_input=user_query)
    else:
        if mode == "Triage":
            bot_reply = get_chat_response(user_query)
        else:  # Chat mode
            bot_reply = get_crag_response(user_query)

    # Display the bot's reply with appropriate formatting
    if st.session_state["in_triage"]:
        # Still in triage -> bot_reply is a follow-up question
        st.markdown(f"**Bot:** {bot_reply}")
    else:
        # No longer in triage (either a normal answer or triage just ended)
        response_text = str(bot_reply)
        # Color-code final triage outcomes by urgency
        if response_text.lower().startswith("emergency care"):
            st.error(response_text)   # Red highlight for emergency
        elif response_text.lower().startswith("urgent care"):
            st.warning(response_text)  # Yellow highlight for urgent care
        elif response_text.lower().startswith("self care") or "non-urgent care" in response_text.lower():
            st.success(response_text)  # Green highlight for self-care/non-urgent
        else:
            # General chat response or untagged text
            st.markdown(f"**Bot:** {response_text}")

        # If a triage session just concluded, display the full Q&A path
        if not st.session_state["in_triage"] and st.session_state["triage_path"]:
            st.markdown("**Triage Q&A Path:**")
            for qa in st.session_state["triage_path"]:
                st.write(f"- **Q:** {qa['question']}  \n  **A:** {qa['answer']}")
            # Clear the recorded path for next conversation
            st.session_state["triage_path"] = []

# Commented out IPython magic to ensure Python compatibility.
# %%writefile dental_chatbot_app.py
# 
# 
# # -*- coding: utf-8 -*-
# """Dental_Chatbot__Pipeline working_mistral1.ipynb
# 
# Automatically generated by Colab.
# 
# Original file is located at
#     https://colab.research.google.com/drive/1CVQ-PeOSlw_GYyne73qGOpAYCHPDJXVR
# 
# # ðŸ¦· Fine-Tuned Dental Chatbot with CRAG and Streamlit UI
# 
# Welcome to the notebook for building a dental health chatbot pipeline using fine-tuned DistilGPT2 and CRAG.
# This notebook includes:
# - Fine-tuning a DistilGPT2 model on Q&A and multi-turn data.
# - Building a FAISS-based CRAG system with fallback to DuckDuckGo.
# - A Streamlit UI setup for interaction.
# """
# 
# !pip install -q sentence-transformers faiss-cpu transformers accelerate bitsandbytes duckduckgo_search datasets streamlit
# 
# !pip install -U sentence-transformers scikit-learn
# 
# hf_token = "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"
# 
# """## Step 1: Load and Prepare Q&A and Multi-turn Dialogue Data"""
# 
# from huggingface_hub import login
# 
# login(token="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")
# 
# !pip uninstall -y bitsandbytes
# !pip install bitsandbytes  # or the latest version from PyPI
# 
# import requests
# import json
# 
# def load_json(url):
#     r = requests.get(url)
#     r.raise_for_status()
#     return r.json()
# 
# def load_jsonl(url):
#     r = requests.get(url)
#     r.raise_for_status()
#     return [json.loads(line) for line in r.text.splitlines() if line]
# 
# from sentence_transformers import SentenceTransformer
# from sklearn.linear_model import LogisticRegression
# from sklearn.model_selection import train_test_split
# from sklearn.metrics import classification_report
# import joblib
# import json
# 
# # âœ… Step 1: Load your merged training data
# with open("merged_symptom_training_data.json", "r") as f:
#     data = json.load(f)
# 
# texts = [item["text"] for item in data]
# labels = [item["label"] for item in data]
# 
# # âœ… Step 2: Encode using Sentence-BERT
# model_name = "all-MiniLM-L6-v2"
# sbert = SentenceTransformer(model_name)
# X = sbert.encode(texts, convert_to_tensor=False)
# 
# # âœ… Step 3: Train/test split
# X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2, stratify=labels, random_state=42)
# 
# # âœ… Step 4: Train classifier
# clf = LogisticRegression(max_iter=1000)
# clf.fit(X_train, y_train)
# 
# # âœ… Step 5: Evaluate
# y_pred = clf.predict(X_test)
# print(classification_report(y_test, y_pred))
# 
# # âœ… Step 6: Save model and encoder
# joblib.dump(clf, "symptom_classifier.pkl")
# sbert.save("symptom_encoder")
# 
# print("âœ… Classifier and encoder saved.")
# 
# import joblib
# from sentence_transformers import SentenceTransformer
# import numpy as np
# 
# # âœ… Load saved model and encoder
# clf = joblib.load("symptom_classifier.pkl")
# sbert = SentenceTransformer("symptom_encoder")
# 
# # âœ… Classifier function
# def classify_symptom(text):
#     embedding = sbert.encode([text])
#     prediction = clf.predict(embedding)[0]
#     probas = clf.predict_proba(embedding)[0]
#     confidence = np.max(probas)
#     return prediction, float(confidence)
# label, score = classify_symptom("My face is swollen and it hurts when I chew.")
# print(f"Predicted symptom: {label} (confidence: {score:.2f})")
# 
# import json
# import requests
# from datasets import Dataset
# 
# # --- Helper functions to fetch data from GitHub ---
# def load_json(url):
#     r = requests.get(url)
#     r.raise_for_status()
#     return r.json()
# 
# def load_jsonl(url):
#     r = requests.get(url)
#     r.raise_for_status()
#     return [json.loads(line) for line in r.text.splitlines() if line]
# 
# # --- URLs to your JSON files hosted on GitHub ---
# base_url = "https://raw.githubusercontent.com/zaidkhan1609/Datasets/main/chatbot/"
# 
# single_turn_urls = {
#     "dental_qa_pairs.json": base_url + "dental_qa_pairs.json",
#     "dental_health_qna.json": base_url + "dental_health_qna.json",
#     "brushing_and_flossing_qa.json": base_url + "brushing_and_flossing_qa.json",
#     "reddit_qa.jsonl": base_url + "reddit_qa.jsonl"
# }
# 
# multi_turn_url = base_url + "multi_turn_dialogues.json"
# 
# # --- Load single-turn QA data ---
# qa_pairs = []
# for fname, url in single_turn_urls.items():
#     if fname.endswith(".jsonl"):
#         qa_pairs.extend(load_jsonl(url))
#     else:
#         qa_pairs.extend(load_json(url))
# 
# # --- Load multi-turn dialogue data ---
# multi_turn_data = load_json(multi_turn_url)
# 
# # --- Format into conversation examples ---
# train_texts = []
# 
# # Single-turn: "User: ...\nBot: ..."
# for item in qa_pairs:
#     question = item['question'].strip()
#     answer = item['answer'].strip()
#     train_texts.append(f"User: {question}\nBot: {answer}")
# 
# # Multi-turn: "User: ...\nBot: ..."
# for dialogue in multi_turn_data:
#     convo_str = ""
#     for turn in dialogue["turns"]:
#         role = turn["role"].capitalize()
#         text = turn["text"].strip()
#         convo_str += f"{role}: {text}\n"
#     train_texts.append(convo_str.strip())
# 
# print(f"Loaded {len(train_texts)} conversation examples for training.")
# 
# # --- Create Hugging Face dataset ---
# dataset = Dataset.from_list([{"text": t} for t in train_texts])
# dataset = dataset.train_test_split(test_size=0.1, seed=42)
# print(f"Training samples: {len(dataset['train'])}, Validation samples: {len(dataset['test'])}")
# 
# import torch
# torch.cuda.empty_cache()
# 
# # Upgrade PEFT to get BitFit support
# !pip install -U peft
# 
# pip install -U bitsandbytes
# 
# # âœ… Install required libraries
# !pip install -q transformers datasets peft accelerate requests
# 
# # âœ… Imports
# import os, json, requests, torch
# from datasets import Dataset
# from transformers import (
#     AutoModelForCausalLM, AutoTokenizer,
#     TrainingArguments, Trainer, DataCollatorForLanguageModeling,
#     BitsAndBytesConfig
# )
# from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
# 
# # âœ… Memory optimization
# os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
# torch.cuda.empty_cache()
# 
# # âœ… Helper functions to load GitHub-hosted files
# def load_json(url):
#     r = requests.get(url)
#     r.raise_for_status()
#     return r.json()
# 
# def load_jsonl(url):
#     r = requests.get(url)
#     r.raise_for_status()
#     return [json.loads(line) for line in r.text.splitlines() if line]
# 
# # âœ… GitHub base path
# base_url = "https://raw.githubusercontent.com/zaidkhan1609/Datasets/main/chatbot/"
# 
# # âœ… Load QA data from GitHub
# qa_pairs = []
# qa_pairs += load_json(base_url + "dental_qa_pairs.json")
# qa_pairs += load_json(base_url + "dental_health_qna.json")
# qa_pairs += load_json(base_url + "brushing_and_flossing_qa.json")
# qa_pairs += load_jsonl(base_url + "reddit_qa.jsonl")
# 
# # âœ… Load multi-turn dialogue from GitHub
# multi_turn_data = load_json(base_url + "multi_turn_dialogues.json")
# 
# # âœ… Create formatted training texts
# train_texts = [f"User: {item['question'].strip()}\nBot: {item['answer'].strip()}" for item in qa_pairs]
# for dialogue in multi_turn_data:
#     convo = "\n".join(f"{turn['role'].capitalize()}: {turn['text'].strip()}" for turn in dialogue["turns"])
#     train_texts.append(convo)
# 
# # âœ… Tokenizer and model setup
# model_id = "mistralai/Mistral-7B-Instruct-v0.2"
# tokenizer = AutoTokenizer.from_pretrained(model_id)
# tokenizer.pad_token = tokenizer.eos_token
# 
# bnb_config = BitsAndBytesConfig(
#     load_in_4bit=True,
#     bnb_4bit_compute_dtype=torch.float16,
#     bnb_4bit_use_double_quant=True,
#     bnb_4bit_quant_type="nf4",
#     llm_int8_enable_fp32_cpu_offload=True
# )
# 
# model = AutoModelForCausalLM.from_pretrained(
#     model_id,
#     device_map="auto",
#     quantization_config=bnb_config,
#     torch_dtype=torch.float16
# )
# model = prepare_model_for_kbit_training(model)
# 
# # âœ… Apply LoRA config
# lora_config = LoraConfig(
#     r=16,
#     lora_alpha=32,
#     lora_dropout=0.1,
#     bias="none",
#     task_type="CAUSAL_LM"
# )
# model = get_peft_model(model, lora_config)
# 
# # âœ… Tokenization
# raw_dataset = Dataset.from_list([{"text": t} for t in train_texts])
# dataset = raw_dataset.train_test_split(test_size=0.1, seed=42)
# 
# def tokenize(batch):
#     return tokenizer(batch["text"], truncation=True, padding="max_length", max_length=512)
# 
# tokenized_dataset = dataset.map(tokenize, batched=True, remove_columns=["text"])
# data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)
# 
# # âœ… Training setup
# training_args = TrainingArguments(
#     output_dir="./phi2_lora_dental",
#     num_train_epochs=3,
#     per_device_train_batch_size=1,
#     gradient_accumulation_steps=4,
#     eval_strategy="epoch",
#     save_strategy="epoch",
#     learning_rate=2e-4,
#     logging_steps=50,
#     fp16=True,
#     save_total_limit=2,
#     report_to="none"
# )
# 
# # âœ… Train
# trainer = Trainer(
#     model=model,
#     tokenizer=tokenizer,
#     args=training_args,
#     data_collator=data_collator,
#     train_dataset=tokenized_dataset["train"],
#     eval_dataset=tokenized_dataset["test"]
# )
# 
# trainer.train()
# 
# # âœ… Save fine-tuned model
# model.save_pretrained("./phi2_lora_dental")
# tokenizer.save_pretrained("./phi2_lora_dental")
# 
# print("âœ… LoRA fine-tuning complete! Model saved to ./phi2_lora_dental")
# 
# 
# 
# """## Step 2: Fine-Tune DistilGPT2"""
# 
# import os
# import json
# import zipfile
# import requests
# from io import BytesIO
# from sentence_transformers import SentenceTransformer
# 
# # âœ… GitHub base URL
# base_url = "https://raw.githubusercontent.com/zaidkhan1609/Datasets/main/chatbot/"
# 
# # âœ… List of knowledge JSON files (hosted on GitHub)
# knowledge_files = [
#     "Tooth%20decay.json",
#     "bleeding_gums.json",
#     "knocked_out_tooth.json",
#     "loose_tooth.json",
#     "sensitive_teeth.json",
#     "swelling_jaw.json",
#     "tooth_pain.json",
#     "vertopal.com_New%20Microsoft%20Word%20Document.json"
# ]
# 
# # âœ… Chunking function
# def chunk_text(text, chunk_size=256, overlap=64):
#     words = text.split()
#     chunks = []
#     for i in range(0, len(words), chunk_size - overlap):
#         chunk = " ".join(words[i:i + chunk_size])
#         if chunk:
#             chunks.append(chunk)
#     return chunks
# 
# # âœ… Helper to load and parse GitHub-hosted JSON
# def extract_text_from_json_url(file_url):
#     r = requests.get(file_url)
#     r.raise_for_status()
#     data = r.json()
#     content_blocks = data[1] if isinstance(data, list) and len(data) > 1 else data
#     texts = []
# 
#     for block in content_blocks:
#         if not isinstance(block, dict) or "t" not in block:
#             continue
#         t = block["t"]
#         if t in ("Para", "Plain", "Header"):
#             inline_elems = block["c"] if t != "Header" else block["c"][2]
#             paragraph_text = ""
#             for inline in inline_elems:
#                 if inline["t"] == "Str":
#                     paragraph_text += inline["c"]
#                 elif inline["t"] in ("Space", "SoftBreak"):
#                     paragraph_text += " "
#                 elif inline["t"] == "LineBreak":
#                     paragraph_text += "\n"
#                 elif inline["t"] == "Code":
#                     paragraph_text += inline["c"][1]
#             paragraph_text = paragraph_text.strip()
#             if paragraph_text:
#                 texts.append(paragraph_text)
#     return texts
# 
# # âœ… Load and extract all GitHub knowledge JSONs with chunking
# knowledge_texts = []
# for fname in knowledge_files:
#     full_url = base_url + fname
#     for text in extract_text_from_json_url(full_url):
#         knowledge_texts.extend(chunk_text(text))  # âœ… CHUNKING applied here
# 
# # âœ… Also extract structured symptom tree info from ZIP
# symptom_zip_url = base_url + "symptom_trees.zip"
# r = requests.get(symptom_zip_url)
# r.raise_for_status()
# 
# with zipfile.ZipFile(BytesIO(r.content)) as z:
#     for filename in z.namelist():
#         if filename.endswith(".json"):
#             data = json.loads(z.read(filename).decode("utf-8"))
#             symptom = data.get("symptom", "").capitalize()
#             flow = data.get("flow", [])
#             if not flow:
#                 continue
#             advice_text = f"For {symptom}: "
#             if len(flow) == 1:
#                 q = flow[0]["question"].rstrip("?")
#                 yes_outcome = flow[0]["yes"]
#                 no_outcome = flow[0]["no"]
#                 condition = q[0].lower() + q[1:]
#                 advice_text += f"If {condition}, then {yes_outcome}. Otherwise, {no_outcome}."
#             else:
#                 for i, step in enumerate(flow):
#                     q = step["question"].rstrip("?")
#                     yes_outcome = step["yes"]
#                     no_outcome = step["no"]
#                     condition = q[0].lower() + q[1:]
#                     if i < len(flow) - 1:
#                         advice_text += f"If {condition}, then {yes_outcome}. Otherwise, "
#                     else:
#                         advice_text += f"If {condition}, then {yes_outcome}. Otherwise, {no_outcome}."
#             knowledge_texts.append(advice_text)
# 
# # âœ… Preview and count
# print(f"âœ… Total knowledge entries: {len(knowledge_texts)}")
# print("ðŸ” Sample entry:\n", knowledge_texts[0][:300], "...")
# 
# import torch
# 
# device = "cuda" if torch.cuda.is_available() else "cpu"
# 
# from sentence_transformers.util import cos_sim
# from duckduckgo_search import DDGS
# 
# def multiquery_variants(query):
#     return [
#         f"What is {query}?",
#         f"Explain: {query}",
#         f"{query} - symptoms and treatment",
#         f"{query} dental advice",
#         f"Causes and remedies for {query}"
#     ]
# 
# def duckduckgo_fallback(query):
#     with DDGS() as ddgs:
#         results = [r["body"] for r in ddgs.text(query, max_results=3)]
#     return "\\n".join(results)
# # âœ… Load embedding model
# from sentence_transformers import SentenceTransformer
# 
# embedding_model = SentenceTransformer("BAAI/bge-base-en-v1.5", device=device)
# 
# # âœ… Precompute knowledge base embeddings
# kb_embeddings = embedding_model.encode(knowledge_texts, convert_to_tensor=True)
# # Save knowledge base texts and embeddings for Streamlit app
# import numpy as np
# 
# with open("knowledge_texts.json", "w") as f:
#     json.dump(knowledge_texts, f, indent=2)
# 
# np.save("kb_embeddings.npy", kb_embeddings.cpu().numpy())
# 
# import torch
# from torch.nn.functional import normalize  # âœ… NEW import
# 
# device = "cuda" if torch.cuda.is_available() else "cpu"
# 
# def get_crag_response(user_query, threshold=0.65):
#     # MultiQuery expansion
#     queries = multiquery_variants(user_query)
# 
#     # Encode queries and move to device
#     query_embeddings = embedding_model.encode(queries, convert_to_tensor=True).to(device)
# 
#     # Normalize and move KB embeddings to same device
#     kb_tensor = torch.tensor(kb_embeddings).to(device)
#     kb_tensor = normalize(kb_tensor, dim=1)  # âœ… normalize all vectors (rows)
# 
#     best_score, best_text = float('-inf'), ""
# 
#     for qe in query_embeddings:
#         qe = normalize(qe.unsqueeze(0), dim=1)  # âœ… normalize query vector
#         scores = torch.matmul(qe, kb_tensor.T).squeeze(0)  # âœ… dot product similarity
#         top_score, top_idx = torch.max(scores, dim=0)
# 
#         if top_score > best_score:
#             best_score = top_score
#             best_text = knowledge_texts[top_idx]
# 
#     # Fallback if similarity is too low
#     if best_score < threshold:
#         context = duckduckgo_fallback(user_query)
#     else:
#         context = best_text
# 
#     # Format prompt for chat model
#     prompt = f"<s>[INST] Context: {context}\n\nUser: {user_query} [/INST]"
#     input_ids = tokenizer(prompt, return_tensors="pt").input_ids.to(device)
# 
#     output = model.generate(input_ids, max_new_tokens=512)
#     decoded = tokenizer.decode(output[0], skip_special_tokens=True)
# 
#     return decoded
# 
# print(get_crag_response("What are the signs of gum disease?"))
# 
# import streamlit as st
# from io import BytesIO
# 
# # âœ… Load SDCEP triage decision trees from GitHub (symptom_trees.zip)
# symptom_zip_url = base_url + "symptom_trees.zip"
# r = requests.get(symptom_zip_url)
# r.raise_for_status()
# symptom_trees = {}
# with zipfile.ZipFile(BytesIO(r.content)) as z:
#     for filename in z.namelist():
#         if filename.endswith(".json"):
#             data = json.loads(z.read(filename).decode("utf-8"))
#             symptom_name = data.get("symptom", "")
#             flow = data.get("flow", [])
#             if flow:
#                 # Store the flow list, using lowercase key for consistent lookup
#                 symptom_trees[symptom_name.lower()] = flow
# 
# # âœ… Function to traverse a symptom decision tree interactively
# def traverse_sdcep_tree(symptom=None, user_input=None):
#     """
#     Traverse the SDCEP symptom tree for the given symptom. If symptom is provided (starting a new triage),
#     it returns the first question for that symptom. If user_input is provided (continuing an ongoing triage),
#     it processes the answer and returns the next question or final advice.
#     """
#     # Starting a new triage
#     if symptom is not None:
#         symptom_key = symptom.lower()
#         if symptom_key not in symptom_trees:
#             # No matching symptom tree found
#             return get_crag_response(user_input or "")  # Fallback to normal response
#         # Initialize session state for the new triage
#         st.session_state["in_triage"] = True
#         st.session_state["current_tree"] = symptom_key
#         st.session_state["triage_index"] = 0
#         st.session_state["triage_path"] = []  # to record Q&A path
#         # Get the first question from the symptom's flow
#         first_question = symptom_trees[symptom_key][0]["question"]
#         st.session_state["last_question"] = first_question
#         return first_question  # Ask the first question to the user
# 
#     # Continuing an existing triage (user_input is the answer to the last question)
#     if not st.session_state.get("in_triage") or st.session_state.get("current_tree") is None:
#         # If we're not actually in a triage flow, just fall back to CRAG
#         return get_crag_response(user_input or "")
# 
#     # Retrieve current state
#     symptom_key = st.session_state["current_tree"]
#     flow = symptom_trees[symptom_key]
#     idx = st.session_state.get("triage_index", 0)
#     if idx >= len(flow):
#         # Index out of range (should not normally happen)
#         st.session_state["in_triage"] = False
#         return get_crag_response(user_input or "")
# 
#     # Process the user's answer (expected yes/no)
#     answer = (user_input or "").strip().lower()
#     # Normalize common affirmative/negative responses
#     yes_answers = {"yes", "y", "yeah", "yep", "yea", "sure"}
#     no_answers  = {"no", "n", "nope", "nah", "negative"}
#     if answer in yes_answers or answer.startswith("y"):
#         user_answer = "yes"
#     elif answer in no_answers or answer.startswith("n"):
#         user_answer = "no"
#     else:
#         # If answer is not recognized as yes/no, prompt again
#         return "Please answer 'yes' or 'no'."
# 
#     # Record this Q&A in the path
#     question_asked = st.session_state.get("last_question", flow[idx]["question"])
#     st.session_state["triage_path"].append({"question": question_asked, "answer": user_answer})
# 
#     # Determine next step based on answer
#     current_node = flow[idx]
#     next_step = current_node.get(user_answer)  # either the text of next question, or final advice tag
#     if next_step is None:
#         # No next step found (shouldn't happen if data is well-formed)
#         st.session_state["in_triage"] = False
#         return "Sorry, I couldn't determine the next step."
# 
#     # Check if next_step is a directive to jump to another pathway
#     if isinstance(next_step, str) and next_step.lower().startswith("go to"):
#         # Example: "Go to Swelling pathway" -> extract "swelling"
#         target = next_step.split("Go to", 1)[1].strip().lower().replace(" pathway", "")
#         # If the target pathway exists, switch to that tree
#         if target in symptom_trees:
#             st.session_state["current_tree"] = target
#             st.session_state["triage_index"] = 0
#             # Ask the first question of the new pathway
#             first_question = symptom_trees[target][0]["question"]
#             st.session_state["last_question"] = first_question
#             return first_question
#         else:
#             # Target pathway not found; end triage
#             st.session_state["in_triage"] = False
#             return f"Unable to continue to the {target.capitalize()} pathway."
# 
#     # Check if next_step is a final outcome (triage decision) or another question
#     # We consider it a final outcome if it contains "Care:" or is exactly "Urgent Care", "Emergency Care", etc.
#     outcome_text = next_step.strip()
#     is_final = False
#     if outcome_text.lower().startswith(("emergency care", "urgent care", "self care", "non-urgent care")) or outcome_text.lower() == "urgent care" or outcome_text.lower() == "urgent care: dental":
#         is_final = True
#     if outcome_text.lower() == "end":
#         is_final = True
#         # Define a user-friendly message for "End"
#         outcome_text = "No urgent issues identified. No immediate care needed."
# 
#     if is_final:
#         # End of triage reached â€“ provide final advice
#         st.session_state["in_triage"] = False  # reset triage state
#         st.session_state["current_tree"] = None
#         st.session_state["triage_index"] = None
#         # Append the final outcome to the path (as the outcome of the last question)
#         st.session_state["triage_path"].append({"question": "Triage Result", "answer": outcome_text})
#         return outcome_text  # This is the final advice/triage result
# 
#     # If not final and not a pathway jump, treat next_step as the text of the next question in the current flow
#     # Find the index of the question matching next_step in the flow list
#     next_index = None
#     next_q_lower = outcome_text.lower().rstrip("?")
#     for i, step in enumerate(flow):
#         # Match question text (case-insensitive, ignoring trailing '?')
#         if step["question"].lower().rstrip("?") == next_q_lower:
#             next_index = i
#             break
#     if next_index is None:
#         # If we couldn't find the next question (data inconsistency), end triage
#         st.session_state["in_triage"] = False
#         return outcome_text  # Return what we have, which might be a message
# 
#     # Move to the next question
#     st.session_state["triage_index"] = next_index
#     next_question = flow[next_index]["question"]
#     st.session_state["last_question"] = next_question
#     # Return the next question to ask the user
#     return next_question
# 
# # âœ… Routing function: decide between triage and general chat response
# def get_chat_response(user_text):
#     """Route the user query to either the triage engine or the general chat (CRAG) response."""
#     # If a triage Q&A session is already in progress, continue it
#     if st.session_state.get("in_triage"):
#         # In the middle of a triage, the user_text is an answer to the last triage question
#         return traverse_sdcep_tree(user_input=user_text)
#     # Otherwise, determine if we should start a new triage or do a normal chat
#     label, confidence = classify_symptom(user_text)
#     label_key = label.lower()
#     if confidence > 0.75 and label_key in symptom_trees:
#         # High confidence that the query matches a symptom category â€“ start triage
#         return traverse_sdcep_tree(symptom=label_key)
#     else:
#         # Low confidence or no matching symptom â€“ use the conversational QA response
#         return get_crag_response(user_text)
# 
# # âœ… Streamlit App Interface
# st.title("Dental Symptom Chatbot")
# 
# # Mode selection toggle
# mode = st.radio("Choose Mode:", ("Chat", "Triage"), index=0)
# 
# # Initialize session state for triage (if not already set)
# if "in_triage" not in st.session_state:
#     st.session_state["in_triage"] = False
#     st.session_state["current_tree"] = None
#     st.session_state["triage_index"] = None
#     st.session_state["triage_path"] = []
# 
# # User input field
# user_query = st.text_input("Enter your question or describe your symptoms:")
# 
# if st.button("Send") and user_query:
#     # Determine response based on mode and state
#     if st.session_state["in_triage"]:
#         # If already in a triage session, the user_query is an answer to the last question
#         bot_reply = traverse_sdcep_tree(user_input=user_query)
#     else:
#         if mode == "Triage":
#             bot_reply = get_chat_response(user_query)
#         else:  # Chat mode
#             bot_reply = get_crag_response(user_query)
# 
#     # Display the bot's reply with appropriate formatting
#     if st.session_state["in_triage"]:
#         # Still in triage -> bot_reply is a follow-up question
#         st.markdown(f"**Bot:** {bot_reply}")
#     else:
#         # No longer in triage (either a normal answer or triage just ended)
#         response_text = str(bot_reply)
#         # Color-code final triage outcomes by urgency
#         if response_text.lower().startswith("emergency care"):
#             st.error(response_text)   # Red highlight for emergency
#         elif response_text.lower().startswith("urgent care"):
#             st.warning(response_text)  # Yellow highlight for urgent care
#         elif response_text.lower().startswith("self care") or "non-urgent care" in response_text.lower():
#             st.success(response_text)  # Green highlight for self-care/non-urgent
#         else:
#             # General chat response or untagged text
#             st.markdown(f"**Bot:** {response_text}")
# 
#         # If a triage session just concluded, display the full Q&A path
#         if not st.session_state["in_triage"] and st.session_state["triage_path"]:
#             st.markdown("**Triage Q&A Path:**")
#             for qa in st.session_state["triage_path"]:
#                 st.write(f"- **Q:** {qa['question']}  \n  **A:** {qa['answer']}")
#             # Clear the recorded path for next conversation
#             st.session_state["triage_path"] = []

!pip install -q pyngrok

import streamlit as st

st.title("Test App")
st.write("Hello from Streamlit running inside Colab!")

from pyngrok import ngrok, conf
conf.get_default().auth_token = "2yxM7GUIDUtTSPSsE2BBz230KwH_27bzCiAAP2Y27UzAEA36w"  # Replace with your token

from pyngrok import ngrok
import threading
import time
import os

# Kill previous tunnels if any
ngrok.kill()

# Start Streamlit in background
def run_streamlit():
    os.system('streamlit run app.py')

thread = threading.Thread(target=run_streamlit)
thread.start()

# Wait for Streamlit to spin up
time.sleep(5)

# Open public URL
public_url = ngrok.connect(8501)
print(f"ðŸ”— Open your app: {public_url}")

